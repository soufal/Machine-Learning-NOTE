## 11月12日打卡：

作业1：写出并解释逻辑回归的损失函数，推导参数$w$的梯度下降公式。

### 逻辑回归：    

Logistic regression是机器学习中最经典的分类方法。在两个类的情况下，通过模型可以得到输入属于1或0的类别。

#### Sigmoid函数：

逻辑回归模型的核心是其服从逻辑斯谛分布，该分布的分布函数为Sigmoid函数：      

$$\sigma(z)=\frac{1}{1+e^{-z}}$$         

其图像如下所示：

![1542089867823](C:\Users\haha\AppData\Local\Temp\1542089867823.png)

* 该S型曲线以点（0，0.5）为中心对称，随着x值的增大对应的$\sigma(z)$ 值将逼近于1。而随着x的减小，对应的$\sigma(z)$ 值将逼近于0。
* 当$\sigma(z)>=0.5$：正类；$\sigma(z)<0.5$：负类。

#### 得分函数：

Sigmoid函数的输入记为z，则逻辑回归的得分函数可以定义为：

$$z=w_0x_0+w_1x_1+...+w_nx_n$$ 

$$z=\mathbf{w}^T\mathbf{x}$$

这表示两个数值向量的对应元素相乘然后全部加起来即得到z值。这里的向量$\mathbf{w}$ 是我们需要寻找的最佳参数（系数）。

#### 代价函数：    

逻辑回归的代价函数一般采用交叉熵：

$$J=-\frac{1}{m}\sum_{i=1}^m[y^{(i)}log\sigma(z)^{(i)}+(1-y^{(i)})log(1-\sigma(z)^{(i)})]$$

其中$z=\mathbf{w}^T\mathbf{x}$,代入上式，可得代价函数的矩阵形式：

$$J=-\frac{1}{m}[Ylog\sigma(\mathbf{w}^T\mathbf{x})+(1-Y)log(1-\sigma(\mathbf{w}^T\mathbf{x}))]$$

#### 梯度下降法：

是一种求解无约束最优化问题的常用方法。是一种迭代算法，每一步需要求解目标函数的梯度向量。

梯度下降法通过选取适当的初值$x^{(0)}$，不断迭代，更新x的值，进行目标函数的极小化，直到收敛。

**由于负梯度方向是使函数值下降最快的方向，在迭代的每一步，以负梯度方向更新x的值，这样的话可以减少函数值。** 

在这里需要求出使得代价函数$J$最小的参数$\mathbf{w}$,因此使用梯度下降法通过$J$对$\mathbf{w}$的偏导数来进行参数$\mathbf{w}$的更新：

$$\mathbf{w}=\mathbf{w}-\alpha\frac{\partial{J}}{\partial{\mathbf{w}}}$$

其中$\alpha$ 为学习因子，即步长 ,$\frac{\partial{J}}{\partial{\mathbf{w}}}$为：

$$\frac{\partial{J}}{\partial{\mathbf{w}}}=\frac{\partial{j}}{\partial{\sigma(\mathbf{w}^T\mathbf{x})}} \cdot\frac{\partial{\sigma(\mathbf{w}^T\mathbf{x})}}{\partial{\mathbf{w}^T}}=\frac{1}{m}\frac{\sigma(\mathbf{w}^T\mathbf{x})-Y}{\sigma(\mathbf{w}^T\mathbf{x})(1-\sigma(\mathbf{w}^T\mathbf{x}))}\cdot\mathbf{x}^T\sigma(\mathbf{w}^T\mathbf{x})(1-\sigma(\mathbf{w}^T\mathbf{x})) \\ =\frac{1}{m}\mathbf{x}^T(Y-\sigma(\mathbf{w}^T\mathbf{x}))=\frac{1}{m}\mathbf{x^T}(Y-\widehat{Y})$$

其中$\widehat{Y}=\sigma(\mathbf{w}^T\mathbf{x})$ .











