## 11月15日打卡：

**作业1**：推导SMO算法

#### 支持向量机（Support Vector Machines，SVM）      

优点：泛化错误率低，计算开销不大，结果易解释。

缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。    

适用数据类型：数值型和标称型数据。

##### 基于最大间隔分离数据：    

*线性可分*：很容易使用一条直线将两类数据分割开。该直线称为**分割超平面**

N-维时，该分割对象可以称为**超平面**，也就是分类的决策边界。    

> 分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据则属于另一个类别。
>
> 如果数据点离决策边界越远，那么其最后的预测结果就越可信。
>
> 点到分割面的距离称为**间隔**。 

**支持向量（support vector）**就是离分割超平面最近的那些点。

因此在这里我们需要做的是最大化支持向量到分割面的距离。

##### 寻找最大间隔：    

如果超平面的形式可以写成$\mathbf{w}^T\mathbf{x}+b$，那么计算一个数据点A到分割超平面的距离：$\frac{|\mathbf{w}^T\mathbf{x}+b|}{||\mathbf{w}||}$ .

###### 分类器求解的优化：     

1. 使用*海维赛德阶跃函数*（即单位阶跃函数）对$\mathbf{w}^T\mathbf{x}+b$ 作用得到$f(\mathbf{w}^T\mathbf{x}+b)$ ,其中当$u<0$ 时$f(u)$ 输出$-1$，反之输出$1$ 。

2. 当计算数据点到分割面的距离并确定分割面的放置位置时，间隔可以通过$label*(\mathbf{w}^T\mathbf{x}+b)$ 来计算。

3. 找到具有最小间隔的数据点后，对间隔进行最大化：    

   $$argmax_{w,b}\{min_n(label\cdot(\mathbf{w}^T\mathbf{x}+b))\cdot\frac{1}{||\mathbf{w}||}\}$$

4. 对于上述的优化问题，给定一个约束条件：$label*(\mathbf{w}^T\mathbf{x}+b)>=1.0$ ，使其变成一个带约束条件的优化问题，可以通过引入一个拉格朗日乘子来改写该公式，并将超平面写成数据点的形式：

   $$max_{\alpha}[\sum_{i=1}^{m}\alpha-\frac{1}{2}\sum_{i,j=1}^{m}label^{(i)}\cdot label^{(j)} \cdot \alpha_i \cdot \alpha_j <x^{(i)},x^{(j)}>]$$

   其约束条件为：

   $$\alpha>=0, 和\sum_{i-1}^{m}\alpha_i \cdot label^{(i)}=0$$

5. 引入一个松弛变量使得约束条件变为：

   $$C>=\alpha>=0, 和\sum_{i-1}^{m}\alpha_i \cdot label^{(i)}=0$$

   这里的常数$C$ 用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0”这两个目标的权重。












